{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/DarcStar-Solutions-Tech/LLM-ToolMaker/blob/main/toolmaker.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install --upgrade pip\n",
        "!pip install openai"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yjnkVur87kjr",
        "outputId": "8f8383bb-1816-42a7-ca0f-fa32ef204e97"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: pip in /usr/local/lib/python3.10/dist-packages (23.1.2)\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting openai\n",
            "  Downloading openai-0.27.7-py3-none-any.whl (71 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m72.0/72.0 kB\u001b[0m \u001b[31m3.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: requests>=2.20 in /usr/local/lib/python3.10/dist-packages (from openai) (2.27.1)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from openai) (4.65.0)\n",
            "Collecting aiohttp (from openai)\n",
            "  Downloading aiohttp-3.8.4-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.0 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.0/1.0 MB\u001b[0m \u001b[31m31.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests>=2.20->openai) (1.26.15)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests>=2.20->openai) (2022.12.7)\n",
            "Requirement already satisfied: charset-normalizer~=2.0.0 in /usr/local/lib/python3.10/dist-packages (from requests>=2.20->openai) (2.0.12)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests>=2.20->openai) (3.4)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->openai) (23.1.0)\n",
            "Collecting multidict<7.0,>=4.5 (from aiohttp->openai)\n",
            "  Downloading multidict-6.0.4-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (114 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m114.5/114.5 kB\u001b[0m \u001b[31m14.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting async-timeout<5.0,>=4.0.0a3 (from aiohttp->openai)\n",
            "  Downloading async_timeout-4.0.2-py3-none-any.whl (5.8 kB)\n",
            "Collecting yarl<2.0,>=1.0 (from aiohttp->openai)\n",
            "  Downloading yarl-1.9.2-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (268 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m268.8/268.8 kB\u001b[0m \u001b[31m28.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting frozenlist>=1.1.1 (from aiohttp->openai)\n",
            "  Downloading frozenlist-1.3.3-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (149 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m149.6/149.6 kB\u001b[0m \u001b[31m16.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting aiosignal>=1.1.2 (from aiohttp->openai)\n",
            "  Downloading aiosignal-1.3.1-py3-none-any.whl (7.6 kB)\n",
            "Installing collected packages: multidict, frozenlist, async-timeout, yarl, aiosignal, aiohttp, openai\n",
            "Successfully installed aiohttp-3.8.4 aiosignal-1.3.1 async-timeout-4.0.2 frozenlist-1.3.3 multidict-6.0.4 openai-0.27.7 yarl-1.9.2\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OSeEID4N9Scn",
        "outputId": "7cdcd212-9aa8-4877-c6c9-a59f0d8dd026"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1GNNeSHK6BbY",
        "outputId": "f89c79d2-b791-4d0b-ece1-f8de9b2b7087"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Enter your OpenAI API key: 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\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "import json\n",
        "import glob\n",
        "import re\n",
        "from multiprocessing import Pool\n",
        "from tqdm import tqdm\n",
        "import torch\n",
        "import pandas as pd\n",
        "import openai\n",
        "import itertools\n",
        "import random\n",
        "openai.api_key = input(\"Enter your OpenAI API key: \")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "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"
      ],
      "metadata": {
        "id": "mp2sEBpx_UHR"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {
        "id": "ia8wn9w46Bbb"
      },
      "outputs": [],
      "source": [
        "def generate(prompt, max_tokens=256, temperature=0.0, model=\"gpt-3.5-turbo\"):\n",
        "    if model in [\"gpt-3.5-turbo\", \"gpt-4\", \"gpt-4-32k\"]:\n",
        "        params = {\n",
        "            \"model\": model,\n",
        "            \"max_tokens\": max_tokens,\n",
        "            \"temperature\": temperature,\n",
        "            \"messages\": [{\"role\": \"user\", \"content\": prompt}]\n",
        "        }\n",
        "        for retry in range(3):\n",
        "            try:\n",
        "                return openai.ChatCompletion.create(**params)[\"choices\"][0][\"message\"][\"content\"]\n",
        "            except:\n",
        "                pass\n",
        "        raise Exception(\"Failed to generate\")\n",
        "    \n",
        "    # For older models, use the completion API with max_tokens=1024\n",
        "    params = {\n",
        "        \"model\": model,\n",
        "        \"max_tokens\": min(max_tokens, 1024),\n",
        "        \"temperature\": temperature,\n",
        "        \"prompt\": prompt\n",
        "    }\n",
        "    for retry in range(3):\n",
        "        try:\n",
        "            return openai.Completion.create(**params)[\"choices\"][0][\"text\"]\n",
        "        except:\n",
        "            pass"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {
        "id": "eGegEwFI6Bbc"
      },
      "outputs": [],
      "source": [
        "def get_task(task):\n",
        "    with open(f\"/content/drive/MyDrive/bhh/{task}.json\") as f:\n",
        "        data = json.load(f)\n",
        "\n",
        "    # For dyck languages task, we need remove the spaces in the inputs to avoid unnecessary issues with tokenization\n",
        "    if task == \"dyck_languages\":\n",
        "        for example in data[\"examples\"]:\n",
        "            desc, input = example[\"input\"].split(\"Input: \")\n",
        "            input = input.replace(\" \", \"\")\n",
        "            example[\"input\"] = f\"{desc}Input: {input}\"\n",
        "            example[\"target\"] = example[\"target\"].replace(\" \", \"\")\n",
        "    \n",
        "    train = []\n",
        "    val = []\n",
        "    test = []\n",
        "    for index in range(len(data['examples'])):\n",
        "        sample = {\n",
        "            'question': data['examples'][index]['input'],\n",
        "            'answer': data['examples'][index]['target'],\n",
        "        }\n",
        "        if index < 5:\n",
        "            train.append(sample)\n",
        "        elif index < 10:\n",
        "            val.append(sample)\n",
        "        else:\n",
        "            test.append(sample)\n",
        "    return train, val, test"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {
        "id": "KCOwzE5-6Bbd"
      },
      "outputs": [],
      "source": [
        "tool_maker_prompt = \"\"\"Please write a generic Python function to solve this type of problems using only standard python libraries. The output of the function can later be converted to the answer (option for multiple choice question). All the function should be wrapped by\n",
        "```python\n",
        "```\"\"\"\n",
        "\n",
        "tool_verification_prompt = \"\"\"Write unit tests to verify the correctness of the function on the questions above using the following format:\n",
        "```python\n",
        "{parse the question into the arguments of the function}\n",
        "{call the function and save the return value in a variable named `ret`}\n",
        "{for multiple choice question, parse the options}\n",
        "{convert the return value `ret` to the answer (if the question is a multiple choice question, convert to an option) and save it in a variable named `ans`, otherwise}\n",
        "{assert ans == the provided answer (if the question is a multiple choice question, assert ans == option)}\n",
        "```\"\"\"\n",
        "\n",
        "tool_wrapper_prompt = \"\"\"Success! The function is correct. We will need to summarize the function and use cases up for further use. Please extract the information from the history in the following format:\n",
        "\n",
        "Here is a function to solve a class of problems:\n",
        "```python\n",
        "{the function, including necessary imports}\n",
        "```\n",
        "\n",
        "Use cases:\n",
        "Question: {question (including options)}\n",
        "Solution:\n",
        "```python\n",
        "{parse the question into the arguments of the function}\n",
        "{call the function and save the return value in a variable named `ret`}\n",
        "{for multiple choice question, parse the options}\n",
        "{convert the return value `ret` to the answer (if the question is a multiple choice question, convert to an option) and save it in a variable named `ans`, otherwise}\n",
        "```\n",
        "Do this for all the questions in the verification step.\n",
        "\"\"\"\n",
        "\n",
        "\n",
        "def tool_making(train, val, train_samples=3, val_samples=3, model=\"gpt-4\", temperature=0.3):\n",
        "    prompt1 = \"\\n\\n\".join([f\"Question: {sample['question']}\\nAnswer: {sample['answer']}\" for sample in train[:train_samples]]) + \"\\n\\n\" + tool_maker_prompt\n",
        "    message = [{\"role\": \"user\", \"content\": prompt1}]\n",
        "\n",
        "    params = {\n",
        "        \"model\": model,\n",
        "        \"max_tokens\": 2048,\n",
        "        \"temperature\": temperature,\n",
        "        \"messages\": message\n",
        "    }\n",
        "\n",
        "    for retry in range(3):\n",
        "        try:\n",
        "            response = openai.ChatCompletion.create(**params)[\"choices\"][0][\"message\"][\"content\"]\n",
        "            message.append({\"role\": \"assistant\", \"content\": response})\n",
        "            tool = \"\\n\\n\".join(re.findall(r\"```python\\n(.*?)```\", response, re.DOTALL))\n",
        "            exec(tool, globals())\n",
        "            break\n",
        "        except Exception as e:\n",
        "            print(\"ERROR: failed to generate tool\", e)\n",
        "            message.append({\"role\": \"user\", \"content\": f\"Failed to execute the function due to the error: {type(e).__name__} {e}. Please fix it and try again.\"})\n",
        "\n",
        "    print(\"Tool:\", message[-1][\"content\"])\n",
        "        \n",
        "    message.append({\"role\": \"assistant\", \"content\": response})\n",
        "    \n",
        "    prompt2 = \"\\n\\n\".join([f\"Question: {sample['question']}\\nAnswer: {sample['answer']}\" for sample in val[:val_samples]]) + \"\\n\\n\" + tool_verification_prompt\n",
        "\n",
        "    message.append({\"role\": \"user\", \"content\": prompt2})\n",
        "\n",
        "    params = {\n",
        "        \"model\": model,\n",
        "        \"max_tokens\": 2048,\n",
        "        \"temperature\": temperature,\n",
        "        \"messages\": message\n",
        "    }\n",
        "\n",
        "    success = False\n",
        "\n",
        "    for retry in range(3):\n",
        "        try:\n",
        "            response = openai.ChatCompletion.create(**params)[\"choices\"][0][\"message\"][\"content\"]\n",
        "            message.append({\"role\": \"assistant\", \"content\": response})\n",
        "            verification = \"\\n\\n\".join(re.findall(r\"```python\\n(.*?)```\", response, re.DOTALL))\n",
        "            exec(tool+\"\\n\"+verification, globals())\n",
        "            success = True\n",
        "            break\n",
        "        except Exception as e:\n",
        "            print(\"ERROR: failed to verify\", e)\n",
        "            message.append({\"role\": \"user\", \"content\": f\"Failed to verify the function due to the error: {type(e).__name__} {e}. Please fix it and try again.\"})\n",
        "\n",
        "    print(\"Verification:\", message[-1][\"content\"])\n",
        "\n",
        "    if success:\n",
        "        message.append({\"role\": \"user\", \"content\": tool_wrapper_prompt})\n",
        "        params = {\n",
        "            \"model\": model,\n",
        "            \"max_tokens\": 2048,\n",
        "            \"temperature\": temperature,\n",
        "            \"messages\": message\n",
        "        }\n",
        "        try:\n",
        "            response = openai.ChatCompletion.create(**params)[\"choices\"][0][\"message\"][\"content\"]\n",
        "            message.append({\"role\": \"assistant\", \"content\": response})\n",
        "            print(\"Wrapper:\", response)\n",
        "        except Exception as e:\n",
        "            print(\"ERROR: failed to generate wrapper\", e)\n",
        "    return tool, verification, success, message"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 464
        },
        "id": "3ojtbG3Z6Bbe",
        "outputId": "64eecbb2-c791-4e62-c480-ccc5b88c3c48"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ERROR: failed to generate tool The model: `gpt-4` does not exist\n",
            "ERROR: failed to generate tool The model: `gpt-4` does not exist\n",
            "ERROR: failed to generate tool The model: `gpt-4` does not exist\n",
            "Tool: Failed to execute the function due to the error: InvalidRequestError The model: `gpt-4` does not exist. Please fix it and try again.\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "UnboundLocalError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mUnboundLocalError\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-28-656f5c016501>\u001b[0m in \u001b[0;36m<cell line: 13>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0mtrain\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mval\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_task\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtask\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 13\u001b[0;31m \u001b[0mtool\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mverification\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msuccess\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmessage\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtool_making\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mval\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_samples\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mval_samples\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-26-a595b140c5be>\u001b[0m in \u001b[0;36mtool_making\u001b[0;34m(train, val, train_samples, val_samples, model, temperature)\u001b[0m\n\u001b[1;32m     56\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Tool:\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmessage\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"content\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     57\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 58\u001b[0;31m     \u001b[0mmessage\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m{\u001b[0m\u001b[0;34m\"role\"\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;34m\"assistant\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"content\"\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mresponse\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     59\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     60\u001b[0m     \u001b[0mprompt2\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"\\n\\n\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34mf\"Question: {sample['question']}\\nAnswer: {sample['answer']}\"\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0msample\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mval\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mval_samples\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m\"\\n\\n\"\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mtool_verification_prompt\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mUnboundLocalError\u001b[0m: local variable 'response' referenced before assignment"
          ]
        }
      ],
      "source": [
        "task = \"schedule_meeting\"\n",
        "#\"tracking_shuffled_objects_five_objects\"\n",
        "#\"tracking_shuffled_objects_seven_objects\"\n",
        "#\"logical_deduction_five_objects\"\n",
        "#\"logical_deduction_seven_objects\"\n",
        "#\"dyck_languages\"\n",
        "#\"word_sorting\"\n",
        "#\"chinese_remainder_theorem\"\n",
        "#\"schedule_meeting\"\n",
        "model = \"gpt-3.5-turbo\"\n",
        "train, val, test = get_task(task)\n",
        "\n",
        "tool, verification, success, message = tool_making(train, val, train_samples=3, val_samples=3, model=model)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UHBls7gx6Bbf"
      },
      "outputs": [],
      "source": [
        "# torch.save(message, f\"/content/drive/MyDrive/tools/{task}.pt\")\n",
        "# Dump to json\n",
        "json.dump(message, open(f\"/content/drive/MyDrive/tools/{task}.json\", \"w\"))"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.8"
    },
    "orig_nbformat": 4,
    "colab": {
      "provenance": [],
      "toc_visible": true,
      "gpuType": "T4",
      "include_colab_link": true
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}